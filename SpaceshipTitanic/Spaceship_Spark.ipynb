{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Spaceship').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----+-------------+----+-----+-----------+---------+------------+------+------+------------------+-----------+\n",
      "|PassengerId|HomePlanet|CryoSleep|Cabin|  Destination| Age|  VIP|RoomService|FoodCourt|ShoppingMall|   Spa|VRDeck|              Name|Transported|\n",
      "+-----------+----------+---------+-----+-------------+----+-----+-----------+---------+------------+------+------+------------------+-----------+\n",
      "|    0001_01|    Europa|    false|B/0/P|  TRAPPIST-1e|39.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|   Maham Ofracculy|      false|\n",
      "|    0002_01|     Earth|    false|F/0/S|  TRAPPIST-1e|24.0|false|      109.0|      9.0|        25.0| 549.0|  44.0|      Juanna Vines|       true|\n",
      "|    0003_01|    Europa|    false|A/0/S|  TRAPPIST-1e|58.0| true|       43.0|   3576.0|         0.0|6715.0|  49.0|     Altark Susent|      false|\n",
      "|    0003_02|    Europa|    false|A/0/S|  TRAPPIST-1e|33.0|false|        0.0|   1283.0|       371.0|3329.0| 193.0|      Solam Susent|      false|\n",
      "|    0004_01|     Earth|    false|F/1/S|  TRAPPIST-1e|16.0|false|      303.0|     70.0|       151.0| 565.0|   2.0| Willy Santantines|       true|\n",
      "|    0005_01|     Earth|    false|F/0/P|PSO J318.5-22|44.0|false|        0.0|    483.0|         0.0| 291.0|   0.0| Sandie Hinetthews|       true|\n",
      "|    0006_01|     Earth|    false|F/2/S|  TRAPPIST-1e|26.0|false|       42.0|   1539.0|         3.0|   0.0|   0.0|Billex Jacostaffey|       true|\n",
      "|    0006_02|     Earth|     true|G/0/S|  TRAPPIST-1e|28.0|false|        0.0|      0.0|         0.0|   0.0|  NULL|Candra Jacostaffey|       true|\n",
      "|    0007_01|     Earth|    false|F/3/S|  TRAPPIST-1e|35.0|false|        0.0|    785.0|        17.0| 216.0|   0.0|     Andona Beston|       true|\n",
      "|    0008_01|    Europa|     true|B/1/P|  55 Cancri e|14.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|    Erraiam Flatic|       true|\n",
      "|    0008_02|    Europa|     true|B/1/P|  TRAPPIST-1e|34.0|false|        0.0|      0.0|        NULL|   0.0|   0.0|    Altardr Flatic|       true|\n",
      "|    0008_03|    Europa|    false|B/1/P|  55 Cancri e|45.0|false|       39.0|   7295.0|       589.0| 110.0| 124.0|     Wezena Flatic|       true|\n",
      "|    0009_01|      Mars|    false|F/1/P|  TRAPPIST-1e|32.0|false|       73.0|      0.0|      1123.0|   0.0| 113.0|      Berers Barne|       true|\n",
      "|    0010_01|     Earth|    false|G/1/S|  TRAPPIST-1e|48.0|false|      719.0|      1.0|        65.0|   0.0|  24.0|    Reney Baketton|      false|\n",
      "|    0011_01|     Earth|    false|F/2/P|  TRAPPIST-1e|28.0|false|        8.0|    974.0|        12.0|   2.0|   7.0|   Elle Bertsontry|       true|\n",
      "|    0012_01|     Earth|    false| NULL|  TRAPPIST-1e|31.0|false|       32.0|      0.0|       876.0|   0.0|   0.0|     Justie Pooles|      false|\n",
      "|    0014_01|      Mars|    false|F/3/P|  55 Cancri e|27.0|false|     1286.0|    122.0|        NULL|   0.0|   0.0|       Flats Eccle|      false|\n",
      "|    0015_01|     Earth|    false|F/4/P|  55 Cancri e|24.0|false|        0.0|      1.0|         0.0|   0.0| 637.0|   Carry Hughriend|      false|\n",
      "|    0016_01|      Mars|     true|F/5/P|  TRAPPIST-1e|45.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|        Alus Upead|       true|\n",
      "|    0017_01|     Earth|    false|G/0/P|  TRAPPIST-1e| 0.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|     Lyde Brighttt|       true|\n",
      "+-----------+----------+---------+-----+-------------+----+-----+-----------+---------+------------+------+------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = spark.read.csv(\"train.csv\", inferSchema=True, header=True)\n",
    "validation_data = spark.read.csv(\"test.csv\", inferSchema=True, header=True)\n",
    "\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+-----------------+-------------+----+-----+-----------+---------+------------+------+------+------------------+-----------+\n",
      "|PassengerId|HomePlanet|CryoSleep|            Cabin|  Destination| Age|  VIP|RoomService|FoodCourt|ShoppingMall|   Spa|VRDeck|              Name|Transported|\n",
      "+-----------+----------+---------+-----------------+-------------+----+-----+-----------+---------+------------+------+------+------------------+-----------+\n",
      "|    0001_01|    Europa|    false|            B/0/P|  TRAPPIST-1e|39.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|   Maham Ofracculy|      false|\n",
      "|    0002_01|     Earth|    false|            F/0/S|  TRAPPIST-1e|24.0|false|      109.0|      9.0|        25.0| 549.0|  44.0|      Juanna Vines|       true|\n",
      "|    0003_01|    Europa|    false|            A/0/S|  TRAPPIST-1e|58.0| true|       43.0|   3576.0|         0.0|6715.0|  49.0|     Altark Susent|      false|\n",
      "|    0003_02|    Europa|    false|            A/0/S|  TRAPPIST-1e|33.0|false|        0.0|   1283.0|       371.0|3329.0| 193.0|      Solam Susent|      false|\n",
      "|    0004_01|     Earth|    false|            F/1/S|  TRAPPIST-1e|16.0|false|      303.0|     70.0|       151.0| 565.0|   2.0| Willy Santantines|       true|\n",
      "|    0005_01|     Earth|    false|            F/0/P|PSO J318.5-22|44.0|false|        0.0|    483.0|         0.0| 291.0|   0.0| Sandie Hinetthews|       true|\n",
      "|    0006_01|     Earth|    false|            F/2/S|  TRAPPIST-1e|26.0|false|       42.0|   1539.0|         3.0|   0.0|   0.0|Billex Jacostaffey|       true|\n",
      "|    0006_02|     Earth|     true|            G/0/S|  TRAPPIST-1e|28.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|Candra Jacostaffey|       true|\n",
      "|    0007_01|     Earth|    false|            F/3/S|  TRAPPIST-1e|35.0|false|        0.0|    785.0|        17.0| 216.0|   0.0|     Andona Beston|       true|\n",
      "|    0008_01|    Europa|     true|            B/1/P|  55 Cancri e|14.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|    Erraiam Flatic|       true|\n",
      "|    0008_02|    Europa|     true|            B/1/P|  TRAPPIST-1e|34.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|    Altardr Flatic|       true|\n",
      "|    0008_03|    Europa|    false|            B/1/P|  55 Cancri e|45.0|false|       39.0|   7295.0|       589.0| 110.0| 124.0|     Wezena Flatic|       true|\n",
      "|    0009_01|      Mars|    false|            F/1/P|  TRAPPIST-1e|32.0|false|       73.0|      0.0|      1123.0|   0.0| 113.0|      Berers Barne|       true|\n",
      "|    0010_01|     Earth|    false|            G/1/S|  TRAPPIST-1e|48.0|false|      719.0|      1.0|        65.0|   0.0|  24.0|    Reney Baketton|      false|\n",
      "|    0011_01|     Earth|    false|            F/2/P|  TRAPPIST-1e|28.0|false|        8.0|    974.0|        12.0|   2.0|   7.0|   Elle Bertsontry|       true|\n",
      "|    0012_01|     Earth|    false|Unknown/0/Unknown|  TRAPPIST-1e|31.0|false|       32.0|      0.0|       876.0|   0.0|   0.0|     Justie Pooles|      false|\n",
      "|    0014_01|      Mars|    false|            F/3/P|  55 Cancri e|27.0|false|     1286.0|    122.0|         0.0|   0.0|   0.0|       Flats Eccle|      false|\n",
      "|    0015_01|     Earth|    false|            F/4/P|  55 Cancri e|24.0|false|        0.0|      1.0|         0.0|   0.0| 637.0|   Carry Hughriend|      false|\n",
      "|    0016_01|      Mars|     true|            F/5/P|  TRAPPIST-1e|45.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|        Alus Upead|       true|\n",
      "|    0017_01|     Earth|    false|            G/0/P|  TRAPPIST-1e| 0.0|false|        0.0|      0.0|         0.0|   0.0|   0.0|     Lyde Brighttt|       true|\n",
      "+-----------+----------+---------+-----------------+-------------+----+-----+-----------+---------+------------+------+------+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fillna_values = {'CryoSleep': False, \n",
    "                  'VIP': False,\n",
    "                  'Age': 0,\n",
    "                  'VRDeck': 0,\n",
    "                  'Cabin': 'Unknown/0/Unknown',\n",
    "                  'Destination': 'Unknown',\n",
    "                  'ShoppingMall': 0,\n",
    "                  'Name': '',\n",
    "                  'RoomService': 0,\n",
    "                  'FoodCourt': 0,\n",
    "                  'HomePlanet': 'Unknown',\n",
    "                  'Spa': 0}\n",
    "\n",
    "validation_data = validation_data.fillna(value=fillna_values)\n",
    "train_data = train_data.fillna(value=fillna_values)\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'Transported']\n"
     ]
    }
   ],
   "source": [
    "print(train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8693\n",
      "4277\n"
     ]
    }
   ],
   "source": [
    "print(train_data.count())\n",
    "print(validation_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create deck, num, side columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(train_data['Cabin'], '/')\n",
    "train_data = train_data.withColumn('Deck', split_col.getItem(0)) \\\n",
    "       .withColumn('Num', split_col.getItem(1)) \\\n",
    "       .withColumn('Side', split_col.getItem(2))\n",
    "\n",
    "split_col = split(validation_data['Cabin'], '/')\n",
    "validation_data = validation_data.withColumn('Deck', split_col.getItem(0)) \\\n",
    "       .withColumn('Num', split_col.getItem(1)) \\\n",
    "       .withColumn('Side', split_col.getItem(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create passenger group column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = split(train_data['PassengerId'], '_')\n",
    "train_data = train_data.withColumn('GroupId', split_col.getItem(0)) \\\n",
    "       .withColumn('GroupNum', split_col.getItem(1)) \n",
    "\n",
    "split_col = split(validation_data['PassengerId'], '_')\n",
    "validation_data = validation_data.withColumn('GroupId', split_col.getItem(0)) \\\n",
    "       .withColumn('GroupNum', split_col.getItem(1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create First and Second name calumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col = split(train_data['Name'], ' ')\n",
    "train_data = train_data.withColumn('FirstName', split_col.getItem(0)) \\\n",
    "       .withColumn('SecondName', split_col.getItem(1)) \n",
    "\n",
    "split_col = split(validation_data['Name'], ' ')\n",
    "validation_data = validation_data.withColumn('FirstName', split_col.getItem(0)) \\\n",
    "       .withColumn('SecondName', split_col.getItem(1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Indexing the 'Deck' column\n",
    "indexer_deck = StringIndexer(inputCol=\"Deck\", outputCol=\"DeckIndex\")\n",
    "train_data = indexer_deck.fit(train_data).transform(train_data)\n",
    "validation_data = indexer_deck.fit(validation_data).transform(validation_data)\n",
    "\n",
    "# Indexing the 'SecondName' column\n",
    "indexer_second_name = StringIndexer(inputCol=\"SecondName\", outputCol=\"SecondNameIndex\")\n",
    "train_data = indexer_second_name.fit(train_data).transform(train_data)\n",
    "validation_data = indexer_second_name.fit(validation_data).transform(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the integer columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "train_data = train_data.withColumn(\"CryoSleep\", col(\"CryoSleep\").cast(\"int\")) \\\n",
    "                       .withColumn(\"Num\", col(\"Num\").cast(\"int\")) \\\n",
    "                       .withColumn(\"GroupId\", col(\"GroupId\").cast(\"int\")) \\\n",
    "                       .withColumn(\"GroupNum\", col(\"GroupNum\").cast(\"int\")) \\\n",
    "                       .withColumn(\"VIP\", col(\"VIP\").cast(\"int\")) \\\n",
    "                       .withColumn(\"Transported\", col(\"Transported\").cast(\"int\"))\n",
    "\n",
    "validation_data = validation_data.withColumn(\"CryoSleep\", col(\"CryoSleep\").cast(\"int\")) \\\n",
    "                       .withColumn(\"Num\", col(\"Num\").cast(\"int\")) \\\n",
    "                       .withColumn(\"GroupId\", col(\"GroupId\").cast(\"int\")) \\\n",
    "                       .withColumn(\"GroupNum\", col(\"GroupNum\").cast(\"int\")) \\\n",
    "                       .withColumn(\"VIP\", col(\"VIP\").cast(\"int\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "# Indexing and encoding for 'HomePlanet'\n",
    "indexer_home = StringIndexer(inputCol=\"HomePlanet\", outputCol=\"HomePlanetIndex\")\n",
    "encoder_home = OneHotEncoder(inputCol=\"HomePlanetIndex\", outputCol=\"HomePlanetVec\")\n",
    "\n",
    "# Indexing and encoding for 'Destination'\n",
    "indexer_dest = StringIndexer(inputCol=\"Destination\", outputCol=\"DestinationIndex\")\n",
    "encoder_dest = OneHotEncoder(inputCol=\"DestinationIndex\", outputCol=\"DestinationVec\")\n",
    "\n",
    "# Indexing and encoding for 'Side'\n",
    "indexer_side = StringIndexer(inputCol=\"Side\", outputCol=\"SideIndex\")\n",
    "encoder_side = OneHotEncoder(inputCol=\"SideIndex\", outputCol=\"SideVec\")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[indexer_home, encoder_home, indexer_dest, encoder_dest, indexer_side, encoder_side])\n",
    "\n",
    "# Fit and transform the data\n",
    "model = pipeline.fit(train_data)\n",
    "train_data = model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'HomePlanet',\n",
       " 'CryoSleep',\n",
       " 'Cabin',\n",
       " 'Destination',\n",
       " 'Age',\n",
       " 'VIP',\n",
       " 'RoomService',\n",
       " 'FoodCourt',\n",
       " 'ShoppingMall',\n",
       " 'Spa',\n",
       " 'VRDeck',\n",
       " 'Name',\n",
       " 'Transported',\n",
       " 'Deck',\n",
       " 'Num',\n",
       " 'Side',\n",
       " 'GroupId',\n",
       " 'GroupNum',\n",
       " 'FirstName',\n",
       " 'SecondName',\n",
       " 'DeckIndex',\n",
       " 'SecondNameIndex',\n",
       " 'HomePlanetIndex',\n",
       " 'HomePlanetVec',\n",
       " 'DestinationIndex',\n",
       " 'DestinationVec',\n",
       " 'SideIndex',\n",
       " 'SideVec']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureColumnNames = [\n",
    " 'CryoSleep',\n",
    " 'Age',\n",
    " 'VIP',\n",
    " 'RoomService',\n",
    " 'FoodCourt',\n",
    " 'ShoppingMall',\n",
    " 'Spa',\n",
    " 'VRDeck',\n",
    " 'Num',\n",
    " 'Side',\n",
    " 'GroupId',\n",
    " 'GroupNum',\n",
    " 'DeckIndex',\n",
    " 'SecondNameIndex',\n",
    " 'HomePlanetVec',\n",
    " 'DestinationVec',\n",
    " 'SideVec'\n",
    " ]\n",
    "\n",
    "# Combine feature columns into a single vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=featureColumnNames,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "assembled_train_data = assembler.transform(train_data)\n",
    "(train, test) = assembled_train_data.randomSplit([0.8, 0.2])\n",
    "layers = [10, 5, 4, 2]\n",
    "\n",
    "# Create the trainer and set its parameters\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    featuresCol='features', \n",
    "    labelCol='label', \n",
    "    maxIter=100, \n",
    "    layers=layers, \n",
    "    blockSize=128, \n",
    "    seed=1234\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
